version: "3.8"

services:
  vllm:
    image: vllm/vllm-openai:latest
    command: --model $LLM_HF_REPO_ID $VLLM_ARGS
    environment:
      #- VLLM_API_KEY=${API_KEY}
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN}
    volumes:
      - ${MODELS_CACHE_DIR:-"./models"}/${LLM_HF_REPO_ID}:/root/.cache/huggingface
    ipc: host
    expose:
      - 8000
    restart: always
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  tei:
    image: ghcr.io/huggingface/text-embeddings-inference:latest
    command: --model-id $EMBEDDINGS_HF_REPO_ID $TEI_ARGS
    environment:
      #- API_KEY=${API_KEY}
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN}
    volumes:
      - ${MODELS_CACHE_DIR:-"./models"}/${EMBEDDINGS_HF_REPO_ID}:/data
    expose:
      - 80
    restart: always
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  fastapi:
    image: vllme/fastapi
    build:
      context: ./fastapi
      dockerfile: Dockerfile
    command: uvicorn app.main:app --host 0.0.0.0 --port 8000 --browser.gatherUsageStats false
    expose:
      - 8000
    restart: always

  nginx:
    image: vllme/nginx
    build:
      context: ./nginx
      dockerfile: Dockerfile
    ports:
      - 8080:8080
